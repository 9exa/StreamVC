from itertools import islice
import torch
import torch.nn as nn
import torch.optim as optim
import einops
from streamvc.model import StreamVC
from streamvc.train.discriminator import Discriminator
from streamvc.train.loss import GeneratorLoss, DiscriminatorLoss, FeatureLoss, ReconstructionLoss
from streamvc.train.encoder_classifier import EncoderClassifier
from streamvc.train.libritts import get_libritts_dataloader
import time
from accelerate import Accelerator, DataLoaderConfiguration
from accelerate.utils import ProjectConfiguration
import os
import argparse

accelerator = Accelerator(log_with="tensorboard",
                          project_config=ProjectConfiguration(
                              project_dir=os.getcwd(),
                              logging_dir=os.path.join(os.getcwd(), "logs")),
                          dataloader_config=DataLoaderConfiguration(split_batches=True))
# TODO: shouldn't we set it to True only if the input size is constant? - it errors without it
torch.backends.cudnn.benchmark = True
# TODO: isn't it enabled by default? we probably don't
# torch.backends.cudnn.enabled = True

NUM_CLASSES = 100
EMBEDDING_DIMS = 64
SAMPLES_PER_FRAME = 320
TRAIN_SPLIT = "train.other.500"
DEV_SPLIT = "dev.clean"
TEST_SPLIT = "test.clean"
DEVICE = accelerator.device


def print_time(s):
    t = time.localtime()
    current_time = time.strftime("%H:%M:%S", t)
    accelerator.print(f"[{current_time}] - {s}", flush=True)


def sizeof_fmt(num, suffix="B"):
    for unit in ("", "K", "M", "G", "T", "P"):
        if abs(num) < 1024.0:
            return f"{num:.1f} {unit}{suffix}"
        num /= 1024.0


def print_cuda_memory(s):
    if accelerator.device.type != "cuda":
        print_time(s)
        return
    free, total = torch.cuda.mem_get_info()
    curr = torch.cuda.memory_allocated()
    peak = torch.cuda.max_memory_allocated()

    size = {
        "allocated": curr,
        "total": total,
        "free": free,
        "peak": peak
    }

    print_time(
        " | ".join(
            map(lambda x: f"{x[0]} {sizeof_fmt(x[1]):8}", size.items()))
        + f" - {s}")


@torch.no_grad()
def get_batch_labels(hubert_model: nn.Module, batch: torch.Tensor) -> torch.Tensor:
    """
    Get hubert output labels for a given audio samples batch.

    :param hubert_model: Hubert model with discrete output labels.
    :param batch: A batch of audio samples.
    :return: The output predictions generated by the Hubert model for the input batch.
    """
    labels = []
    for sample in batch:
        single_sample_batch = einops.rearrange(sample, 's -> 1 1 s')
        labels.append(hubert_model.units(single_sample_batch))
    return torch.stack(labels, dim=0)


def train_content_encoder(content_encoder: nn.Module, hubert_model: nn.Module, args: argparse.Namespace) -> nn.Module:
    """
    Train a content encoder as a classifier to predict the same labels as a discrete hubert model.

    :param content_encoder: A content encoder wrapped with a linear layer to
    :param hubert_model: Hubert model with discrete output labels.
    :param lr: Learning rate.
    :param num_epochs: Number of epochs.
    :return: The trained content encoder wrapped with a linear layer for classification.
    """
    # TODO: add epochs or number of steps when we know how much time it takes to train the model.
    wrapped_content_encoder = EncoderClassifier(
        content_encoder, EMBEDDING_DIMS, NUM_CLASSES).train()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(
        wrapped_content_encoder.parameters(),
        lr=args.lr,
        betas=args.betas,
        eps=args.eps,
        weight_decay=args.weight_decay,
    )
    dataloader = get_libritts_dataloader(
        TRAIN_SPLIT, args.batch_size, limit_samples=args.limit_batch_samples)

    [
        wrapped_content_encoder,
        optimizer,
        dataloader,
        criterion
    ] = accelerator.prepare(
        wrapped_content_encoder,
        optimizer,
        dataloader,
        criterion
    )

    # TODO: distributed inference with the hubert model
    hubert_model.to(accelerator.device)
    costs = []
    for epoch in range(0, args.num_epochs):
        print_time(f"epoch num: {epoch}")
        for step, batch in enumerate(islice(dataloader, args.limit_num_batches)):

            labels = get_batch_labels(hubert_model, batch)
            with accelerator.accumulate(wrapped_content_encoder):
                outputs = wrapped_content_encoder(batch)
                outputs_flat = outputs.view(-1, NUM_CLASSES)
                labels_flat = labels.view(-1)
                loss = criterion(outputs_flat, labels_flat)
                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()
                accelerator.log(
                    {
                        "loss/content_encoder": loss.item(),
                        "allocated_memory": torch.cuda.max_memory_allocated()
                        if accelerator.device.type == "cuda"
                        else 0
                    },
                    step=epoch*step+step)
                costs.append(loss.item())

            # print loss
            if (step + 1) % args.log_interval == 0:
                print_time(
                    f'[{epoch}, {step:5}] loss: {torch.tensor(costs).mean().item():.4}')
                costs = []
            # save model checkpoints
            if (step + 1) % args.model_checkpoint_interval == 0:
                accelerator.save_model(
                    wrapped_content_encoder,
                    save_directory=os.path.join(
                        args.checkpoint_path,
                        f"{args.run_name}_content_encoder_{epoch}_{step}"
                    ))
            # compute accuracy on main process
            if (step + 1) % args.accuracy_interval == 0:
                if accelerator.is_main_process:
                    accuracy = compute_content_encoder_accuracy(
                        wrapped_content_encoder, hubert_model, dev=True)
                    accelerator.log(
                        {
                            "accuracy/content_encoder": accuracy.item()
                        },
                        step=epoch*step+step)
                    print_time(f"accuracy: {accuracy:.2f}%")

            if accelerator.device.type == "cuda":
                torch.cuda.reset_peak_memory_stats()

    return wrapped_content_encoder


@torch.no_grad()
def compute_content_encoder_accuracy(wrapped_content_encoder: nn.Module, hubert_model: nn.Module, dev=False):
    correct = 0
    total = 0
    if dev:
        dataloader = islice(get_libritts_dataloader(DEV_SPLIT, 64), 200)
    else:
        dataloader = get_libritts_dataloader(TEST_SPLIT, 64)
    wrapped_content_encoder.to(accelerator.device).eval()
    for batch in dataloader:
        batch = batch.to(accelerator.device)
        labels = get_batch_labels(hubert_model, batch)
        outputs = wrapped_content_encoder(batch)
        outputs_flat = outputs.view(-1, NUM_CLASSES)
        labels_flat = labels.view(-1)
        _, predicted = torch.max(outputs_flat.data, 1)
        total += labels_flat.size(0)
        correct += (predicted == labels_flat).sum().item()

    wrapped_content_encoder.to(accelerator.device).train()

    return 100 * correct / total


def train_streamvc(streamvc_model: StreamVC, args: argparse.Namespace) -> None:
    """
       Trains a StreamVC model.

       :param streamvc_model: The model to train.
       :param args: Hyperparameters for training.
       """
    #######################
    # Load PyTorch Models #
    #######################
    # TODO: When we finish implementing StreamVC, replace AdditiveModule with streamvc.
    generator = streamvc_model
    discriminator = Discriminator()

    #####################
    # Create optimizers #
    #####################
    optimizer_generator = torch.optim.Adam(
        generator.parameters(), lr=1e-4, betas=(0.5, 0.9))
    optimizer_discriminator = torch.optim.Adam(
        discriminator.parameters(), lr=1e-4, betas=(0.5, 0.9))

    dataloader = get_libritts_dataloader(
        TRAIN_SPLIT, args.batch_size, limit_samples=args.limit_batch_samples)

    generator_loss_fn = GeneratorLoss()
    discriminator_loss_fn = DiscriminatorLoss()
    feature_loss_fn = FeatureLoss()
    reconstruction_loss_fn = ReconstructionLoss()

    [
        generator,
        discriminator,
        optimizer_generator,
        optimizer_discriminator,
        dataloader,
        generator_loss_fn,
        discriminator_loss_fn,
        feature_loss_fn,
        reconstruction_loss_fn
    ] = accelerator.prepare(
        generator,
        discriminator,
        optimizer_generator,
        optimizer_discriminator,
        dataloader,
        generator_loss_fn,
        discriminator_loss_fn,
        feature_loss_fn,
        reconstruction_loss_fn

    )

    ##########################
    # Dumping original audio #
    ##########################

    costs = []
    for epoch in range(0, args.num_epochs):
        print_time(f"epoch num: {epoch}")
        for step, batch in enumerate(islice(dataloader, args.limit_num_batches)):
            with accelerator.accumulate(generator, discriminator):
                x_pred_t = generator(batch, batch)
                x_pred_t = x_pred_t.unsqueeze(1)
                batch = batch.unsqueeze(1)
                # Remove the first 2 frames from the generated audio
                # because we match a output frame t with input frame t-2.
                x_pred_t = x_pred_t[..., SAMPLES_PER_FRAME * 2:]
                batch = batch[..., :x_pred_t.shape[-1]]

                #######################
                # Train Discriminator #
                #######################

                discriminator_fake_detached = discriminator(x_pred_t.detach())
                discriminator_real = discriminator(batch)

                discriminator_loss = discriminator_loss_fn(
                    discriminator_real, discriminator_fake_detached)

                ###################
                # Train Generator #
                ###################
                discriminator_fake = discriminator(x_pred_t)

                # Compute adversarial loss.
                adversarial_loss = generator_loss_fn(discriminator_fake)

                # Compute feature loss.
                feature_loss = feature_loss_fn(
                    discriminator_real, discriminator_fake)

                # Compute reconstruction loss.
                reconstruction_loss = reconstruction_loss_fn(batch, x_pred_t)

                generator.zero_grad()
                discriminator.zero_grad()
                losses = (
                    discriminator_loss +
                    args.lambda_adversarial * adversarial_loss +
                    args.lambda_feature * feature_loss +
                    args.lambda_reconstruction * reconstruction_loss)
                accelerator.backward(losses)
                optimizer_discriminator.step()
                optimizer_generator.step()

                ######################
                # Update tensorboard #
                ######################
                costs.append([discriminator_loss.item(),
                              adversarial_loss.item(), feature_loss.item()])

                accelerator.log(
                    {
                        "loss/discriminator": discriminator_loss.item(),
                        "loss/adversarial": adversarial_loss.item(),
                        "loss/feature_matching": feature_loss.item(),
                        "loss/reconstruction": reconstruction_loss.item(),
                        "allocated_memory": torch.cuda.max_memory_allocated()
                        if accelerator.device.type == "cuda"
                        else 0
                    },
                    step=epoch*step+step)

                # TODO save checkpoint.
                if (step + 1) % args.log_interval == 0:
                    print_time(
                        f'[{epoch}, {step:5}] loss: {torch.tensor(costs).mean().item():.4}')
                    costs = []
                if (step + 1) % args.model_checkpoint_interval == 0:
                    accelerator.save_model(
                        generator,
                        save_directory=os.path.join(
                            args.checkpoint_path,
                            f"{args.run_name}_generator_{epoch}_{step}"
                        ))
                    accelerator.save_model(
                        discriminator,
                        save_directory=os.path.join(
                            args.checkpoint_path,
                            f"{args.run_name}_discriminator_{epoch}_{step}"
                        ))
            if accelerator.device.type == "cuda":
                torch.cuda.reset_peak_memory_stats()


def main(args):
    """Main function for training StreamVC model."""
    print_time(
        f"DEVICE={accelerator.device} " +
        f"mixed_precision={accelerator.mixed_precision} " +
        f"checkpoints={args.checkpoint_path}")
    hps = {
        "batch_size": args.batch_size,
        "num_epochs": args.num_epochs,
        "lr": args.lr,
        "beta0": args.betas[0],
        "beta1": args.betas[1],
        "eps": args.eps,
        "weight_decay": args.weight_decay,
        "gradient_accumulation_steps": accelerator.gradient_accumulation_steps
    }
    print_time(f"{hps=}")
    accelerator.init_trackers(args.run_name, config=hps)
    streamvc = StreamVC(
        gradient_checkpointing=args.gradient_checkpointing)
    if args.module_to_train in ["content-encoder", "all"]:
        content_encoder = streamvc.content_encoder
        hubert_model = torch.hub.load("bshall/hubert:main", "hubert_discrete",
                                      trust_repo=True).to(torch.float32).eval()
        wrapped_content_encoder = train_content_encoder(
            content_encoder, hubert_model, args)
        accuracy = compute_content_encoder_accuracy(
            wrapped_content_encoder, hubert_model, args)
        print_time(f"{accuracy=}")
    # else:
        # TODO: load content encoder checkpoint

    if args.module_to_train in ["decoder-and-speaker", "all"]:
        train_streamvc(streamvc, args)

    accelerator.end_training()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--run-name", type=str, default="streamvc")
    parser.add_argument("--batch-size", type=int, default=24)
    parser.add_argument("--limit-num-batches", type=int, default=None)
    parser.add_argument("--limit-batch-samples", type=int, default=16_000 * 20)
    parser.add_argument("--num-epochs", type=int, default=1)
    parser.add_argument("--lr", type=float, default=0.001)
    parser.add_argument("--checkpoint-path", type=str,
                        default=os.path.join(
                            os.environ.get("HF_HOME", os.getcwd()),
                            "checkpoints"))
    parser.add_argument("--betas", type=float, nargs=2, default=(0.9, 0.98))
    parser.add_argument("--eps", type=float, default=1e-06)
    parser.add_argument("--weight-decay", type=float, default=1e-2)
    parser.add_argument("--no-gradient-checkpointing",
                        action="store_false", dest='gradient_checkpointing',
                        default=True)
    parser.add_argument("--log-interval", type=int, default=20)
    parser.add_argument("--model-checkpoint-interval", type=int, default=100)
    parser.add_argument("--lambda-feature", type=float, default=100)
    parser.add_argument("--lambda-reconstruction", type=float, default=1)
    parser.add_argument("--lambda-adversarial", type=float, default=1)
    parser.add_argument("--content-encoder-checkpoint", type=str, default="")
    parser.add_argument("--module-to-train", type=str,
                        choices=["content-encoder", "decoder-and-speaker", "all"], required=True)
    parser.add_argument("--accuracy-interval", type=int, default=100)

    args = parser.parse_args()

    if args.module_to_train == "decoder-and-speaker":
        assert args.content_encoder_checkpoint, "content-encoder-checkpoint is required for decoder training"

    main(args)
