from itertools import islice
import torch
import torch.nn as nn
import torch.optim as optim
import einops
from streamvc.model import StreamVC
from streamvc.train.encoder_classifier import EncoderClassifier
from streamvc.train.libritts import get_libritts_dataloader
import time
from accelerate import Accelerator
from accelerate.utils import ProjectConfiguration
import os
import argparse

accelerator = Accelerator(log_with="tensorboard",
                          project_config=ProjectConfiguration(
                              project_dir=os.getcwd(),
                              logging_dir=os.path.join(os.getcwd(), "logs"),))
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.enabled = True

NUM_CLASSES = 100
EMBEDDING_DIMS = 64
TRAIN_SPLIT = "train.other.500"
TEST_SPLIT = "test.clean"
DEVICE = accelerator.device


def print_time(s):
    t = time.localtime()
    current_time = time.strftime("%H:%M:%S", t)
    accelerator.print(f"[{current_time}] - {s}", flush=True)


def sizeof_fmt(num, suffix="B"):
    for unit in ("", "K", "M", "G", "T", "P"):
        if abs(num) < 1024.0:
            return f"{num:.1f} {unit}{suffix}"
        num /= 1024.0


def print_cuda_memory(s):
    if accelerator.device.type != "cuda":
        print_time(s)
        return
    free, total = torch.cuda.mem_get_info()
    curr = torch.cuda.memory_allocated()
    peak = torch.cuda.max_memory_allocated()

    size = {
        "allocated": curr,
        "total": total,
        "free": free,
        "peak": peak
    }

    print_time(
        " | ".join(
            map(lambda x: f"{x[0]} {sizeof_fmt(x[1]):8}", size.items()))
        + f" - {s}")


@torch.no_grad()
def get_batch_labels(hubert_model: nn.Module, batch: torch.Tensor) -> torch.Tensor:
    """
    Get hubert output labels for a given audio samples batch.

    :param hubert_model: Hubert model with discrete output labels.
    :param batch: A batch of audio samples.
    :return: The output predictions generated by the Hubert model for the input batch.
    """
    labels = []
    device = next(hubert_model.parameters()).device
    batch = batch.to(device=device)
    for sample in batch:
        single_sample_batch = einops.rearrange(sample, 's -> 1 1 s')
        labels.append(hubert_model.units(single_sample_batch))
    return torch.stack(labels, dim=0)


def train_content_encoder(content_encoder: nn.Module, hubert_model: nn.Module, args: argparse.Namespace) -> nn.Module:
    """
    Train a content encoder as a classifier to predict the same labels as a discrete hubert model.

    :param content_encoder: A content encoder wrapped with a linear layer to
    :param hubert_model: Hubert model with discrete output labels.
    :param lr: Learning rate.
    :param num_epochs: Number of epochs.
    :return: The trained content encoder wrapped with a linear layer for classification.
    """
    # TODO: add epochs or number of steps when we know how much time it takes to train the model.
    wrapped_content_encoder = EncoderClassifier(
        content_encoder, EMBEDDING_DIMS, NUM_CLASSES).train()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(
        wrapped_content_encoder.parameters(),
        lr=args.lr,
        betas=args.betas,
        eps=args.eps,
        weight_decay=args.weight_decay,
    )
    dataloader = get_libritts_dataloader(
        TRAIN_SPLIT, args.batch_size, limit_samples=args.limit_batch_samples)

    [
        wrapped_content_encoder,
        optimizer,
        dataloader,
        criterion
    ] = accelerator.prepare(
        wrapped_content_encoder,
        optimizer,
        dataloader,
        criterion
    )

    for epoch in range(0, args.num_epochs):
        print_time(f"epoch num: {epoch}")
        for step, batch in enumerate(islice(dataloader, args.limit_num_batches)):
            hubert_model.to(accelerator.device)
            labels = get_batch_labels(hubert_model, batch)
            hubert_model.to(torch.device("cpu"))
            with accelerator.accumulate(wrapped_content_encoder):
                outputs = wrapped_content_encoder(batch)
                outputs_flat = outputs.view(-1, NUM_CLASSES)
                labels_flat = labels.view(-1)
                loss = criterion(outputs_flat, labels_flat)
                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()
                accelerator.log(
                    {
                        "training_loss": loss.item(),
                        "allocated_memory": torch.cuda.max_memory_allocated()
                        if accelerator.device.type == "cuda"
                        else 0
                    },
                    step=epoch*step+step)
                if step % 20 == 0:
                    print_time(f'[{epoch}, {step:5}] loss: {loss.item():.4}')
            if accelerator.device.type == "cuda":
                torch.cuda.reset_peak_memory_stats()

    return wrapped_content_encoder


@ torch.no_grad()
def compute_content_encoder_accuracy(wrapped_content_encoder: nn.Module, hubert_model: nn.Module, args: argparse.Namespace):
    correct = 0
    total = 0
    dataloader = get_libritts_dataloader(TEST_SPLIT, args.batch_size)
    wrapped_content_encoder.to(accelerator.device).eval()
    for batch in dataloader:
        batch = batch.to(accelerator.device)
        hubert_model.to(accelerator.device)
        labels = get_batch_labels(hubert_model, batch)
        outputs = wrapped_content_encoder(batch)
        outputs_flat = outputs.view(-1, NUM_CLASSES)
        labels_flat = labels.view(-1)
        _, predicted = torch.max(outputs_flat.data, 1)
        total += labels_flat.size(0)
        correct += (predicted == labels_flat).sum().item()

    print_time(f'Accuracy of the network: {100 * correct / total} %')
    accelerator.log({"accuracy": 100 * correct / total})


def main(args):
    """Main function for training StreamVC model."""
    print_time(
        f"DEVICE={accelerator.device} " +
        f"mixed_precision={accelerator.mixed_precision} " +
        f"checkpoints={args.checkpoint_path}")
    hps = {
        "batch_size": args.batch_size,
        "num_epochs": args.num_epochs,
        "lr": args.lr,
        "beta0": args.betas[0],
        "beta1": args.betas[1],
        "eps": args.eps,
        "weight_decay": args.weight_decay,
        "gradient_accumulation_steps": accelerator.gradient_accumulation_steps
    }
    print_time(f"{hps=}")
    accelerator.init_trackers(args.run_name, config=hps)
    streamvc = StreamVC(gradient_checkpointing=args.gradient_checkpointing)
    content_encoder = streamvc.content_encoder.train()
    hubert_model = torch.hub.load("bshall/hubert:main", "hubert_discrete",
                                  trust_repo=True).to(torch.float32).eval()
    wrapped_content_encoder = train_content_encoder(
        content_encoder, hubert_model, args)
    compute_content_encoder_accuracy(
        wrapped_content_encoder, hubert_model, args)

    accelerator.end_training()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--run-name", type=str, default="streamvc")
    parser.add_argument("--batch-size", type=int, default=24)
    parser.add_argument("--limit-num-batches", type=int, default=None)
    parser.add_argument("--limit-batch-samples", type=int, default=16_000 * 20)
    parser.add_argument("--num-epochs", type=int, default=1)
    parser.add_argument("--lr", type=float, default=0.001)
    parser.add_argument("--checkpoint-path", type=str,
                        default=os.path.join(
                            os.environ.get("HF_HOME", os.getcwd()),
                            "checkpoints"))
    parser.add_argument("--betas", type=float, nargs=2, default=(0.9, 0.98))
    parser.add_argument("--eps", type=float, default=1e-06)
    parser.add_argument("--weight-decay", type=float, default=1e-2)
    parser.add_argument("--no-gradient-checkpointing",
                        action="store_false", dest='gradient_checkpointing',
                        default=True)

    args = parser.parse_args()
    main(args)
